{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNw52evqoiOtTdYYDiZ14sk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tstrebe2/predicting-text-difficulty/blob/tim-updates/notebooks/pipelines/final-competition-pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "!{sys.executable} -m pip install pyspark==3.1.2 -q\n",
        "!{sys.executable} -m pip install spark-nlp==4.2.0 -q"
      ],
      "metadata": {
        "id": "R6Z4jjikrL1o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d69cae4-0d55-4828-8f0c-ee0b13873398"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 212.4 MB 63 kB/s \n",
            "\u001b[K     |████████████████████████████████| 198 kB 20.1 MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 641 kB 4.2 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StringType\n",
        "from pyspark.ml import Pipeline\n",
        "import pyspark.sql.functions as F\n",
        "from sparknlp.base import DocumentAssembler, EmbeddingsFinisher\n",
        "from sparknlp.annotator import SentenceDetector, Tokenizer, Normalizer, Lemmatizer\n",
        "import sparknlp\n",
        "from pyspark.sql.types import StringType, ArrayType, FloatType, StructType\n",
        "import numpy as np\n",
        "import string\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "sWsZSCvaR0sM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "!wget https://raw.githubusercontent.com/Tstrebe2/predicting-text-difficulty/main/assets/WikiLarge_Train.csv -q\n",
        "!wget https://raw.githubusercontent.com/Tstrebe2/predicting-text-difficulty/main/assets/WikiLarge_Test.csv -q\n",
        "!wget https://raw.githubusercontent.com/mahavivo/vocabulary/master/lemmas/AntBNC_lemmas_ver_001.txt -q\n",
        "!wget https://raw.githubusercontent.com/Tstrebe2/predicting-text-difficulty/main/assets/dale_chall.txt -q\n",
        "!wget https://raw.githubusercontent.com/Tstrebe2/predicting-text-difficulty/main/assets/AoA_51715_words.csv -q\n",
        "!wget https://raw.githubusercontent.com/Tstrebe2/predicting-text-difficulty/main/assets/Concreteness_ratings_Brysbaert_et_al_BRM.txt -q\n",
        "# !wget http://nlp.stanford.edu/data/glove.6B.zip -q\n",
        "# !unzip glove*.zip"
      ],
      "metadata": {
        "id": "yY036731rSpx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:4.2.0\")\\\n",
        "        .getOrCreate()"
      ],
      "metadata": {
        "id": "Lu6IvgEzSATw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Readability Datasets (Provided by MADS)"
      ],
      "metadata": {
        "id": "U07c08R0bxDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_readability_datasets():\n",
        "  aoa = pd.read_csv('/content/AoA_51715_words.csv', \n",
        "                      encoding_errors='ignore', \n",
        "                      usecols=['Lemma_highest_PoS', 'AoA_Kup_lem'],\n",
        "                      ).rename({'Lemma_highest_PoS':'lemma', 'AoA_Kup_lem':'aoa'}, axis=1)\n",
        "\n",
        "  aoa = aoa.groupby('lemma').first().to_dict()['aoa']\n",
        "\n",
        "  conc = (pd.read_csv('/content/Concreteness_ratings_Brysbaert_et_al_BRM.txt', \n",
        "                    sep='\\t',\n",
        "                    usecols=['Word', 'Bigram', 'Conc.M'])\n",
        "          .rename({'Word':'word', 'Bigram':'bigram', 'Conc.M':'conc_mean'}, axis=1))\n",
        "\n",
        "  def split_word(x):\n",
        "    if x['bigram'] == 0:\n",
        "      word_or_phrase = x['word']\n",
        "    else:\n",
        "      word_or_phrase = tuple(x['word'].split(' '))\n",
        "\n",
        "    return {'word':word_or_phrase, 'conc_mean':x['conc_mean'] }\n",
        "\n",
        "  conc = conc.apply(split_word, axis=1, result_type='expand').set_index('word').to_dict()['conc_mean']\n",
        "\n",
        "  d_chall = set(pd.read_csv('/content/dale_chall.txt', names=['word'])['word'].tolist())\n",
        "  return aoa, conc, d_chall"
      ],
      "metadata": {
        "id": "5w0HXmK1bOCY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Core Datasets\n",
        "* Standars unit-conversion metrics\n",
        "* Clean punctuation\n",
        "* Lower-case\n",
        "* Lemmatize\n",
        "* Create readability feature representations using MADS provided datasets\n"
      ],
      "metadata": {
        "id": "OOEVdvltb27o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_clean_data_frame(data_path):\n",
        "  aoa, conc, d_chall = get_readability_datasets()\n",
        "\n",
        "  df = spark.read.csv(data_path, header=True)\n",
        "  df.createOrReplaceTempView('wiki')\n",
        "\n",
        "  regex1, replace1 = r\" km \", \"kilometers\"\n",
        "  regex2, replace2 = r\"[0-9]+(km) \", \"kilometers\"\n",
        "  regex3, replace3 = r\" mph \",\" miles per hour \"\n",
        "  regex4, replace4 = r\"° C \",\"degrees celsius\"\n",
        "  regex5, replace5 = r\"° F \",\"degrees farenheit\"\n",
        "  regex6, replace6 = r\"°\",\"degrees\"\n",
        "  regex7, replace7 = r\" %\",\" percent\"\n",
        "  regex8, replace8 = r\" cm\",\" centimeters\"\n",
        "  regex9, replace9 = r\" kg \",\" kilograms \"\n",
        "\n",
        "  iterable = ((regex1, replace1), (regex2, replace2), (regex3, replace3),\n",
        "              (regex4, replace4), (regex5, replace5), (regex6, replace6),\n",
        "              (regex7, replace7), (regex8, replace8), (regex9, replace9),)\n",
        "\n",
        "  for regex, replace in iterable:\n",
        "    query = f\"\"\"\n",
        "    SELECT\n",
        "      regexp_replace(original_text, '{regex}', '{replace}') as original_text, \n",
        "      label \n",
        "    FROM wiki;\"\"\"\n",
        "    df = spark.sql(query)\n",
        "    df.createOrReplaceTempView('wiki')\n",
        "\n",
        "  documentAssembler = DocumentAssembler()\\\n",
        "      .setInputCol(\"original_text\")\\\n",
        "      .setOutputCol(\"document\")\n",
        "\n",
        "  tokenizer = Tokenizer() \\\n",
        "      .setInputCols([\"document\"]) \\\n",
        "      .setOutputCol(\"token\")\n",
        "\n",
        "  normalizer = Normalizer() \\\n",
        "      .setInputCols([\"token\"]) \\\n",
        "      .setOutputCol(\"normalized\") \\\n",
        "      .setLowercase(True) \\\n",
        "      .setCleanupPatterns([\"\"\"[^\\w\\d\\s]\"\"\"]) # remove punctuations (keep alphanumeric chars)\n",
        "  # if we don't set CleanupPatterns, it will only keep alphabet letters ([^A-Za-z])\n",
        "\n",
        "  lemmatizer = Lemmatizer() \\\n",
        "      .setInputCols([\"normalized\"]) \\\n",
        "      .setOutputCol(\"lemma\") \\\n",
        "      .setDictionary(\"./AntBNC_lemmas_ver_001.txt\", value_delimiter =\"\\t\", key_delimiter = \"->\")\n",
        "\n",
        "  nlp_pipeline = Pipeline(stages=[documentAssembler,\n",
        "                                  tokenizer,\n",
        "                                  normalizer,\n",
        "                                  lemmatizer,])\n",
        "\n",
        "  nlp_pipeline = nlp_pipeline.fit(df)\n",
        "  df = nlp_pipeline.transform(df)\n",
        "  df.createOrReplaceTempView('wiki')\n",
        "\n",
        "  def get_d_chall(x):\n",
        "    easy_count = 0\n",
        "    word_count = 0\n",
        "\n",
        "    for token in x:\n",
        "      if token in string.punctuation:\n",
        "        continue\n",
        "\n",
        "      token_lower = token.lower()\n",
        "\n",
        "      if token_lower in d_chall:\n",
        "        easy_count += 1\n",
        "\n",
        "      word_count += 1\n",
        "\n",
        "    difficult_count = (word_count-easy_count)\n",
        "\n",
        "    return (0.1579 * ((difficult_count/word_count) * 100) + 0.0496 * word_count\n",
        "            if word_count else 0.0)\n",
        "\n",
        "\n",
        "  def get_aoa(x):\n",
        "    arr = [aoa[w.lower()] for w in x if w.lower() in aoa]\n",
        "    if len(arr) > 0:\n",
        "      return arr\n",
        "    else:\n",
        "      return [0.0] \n",
        "\n",
        "  def get_conc_rating(x):\n",
        "    ret_val = []\n",
        "\n",
        "    bigrams = [(f.lower(), s.lower()) for f, s in zip(x[:-1], x[1:])]\n",
        "    cont = False\n",
        "    \n",
        "    for bigram in bigrams:\n",
        "      if cont:\n",
        "        cont = False\n",
        "        continue\n",
        "\n",
        "      if bigram in conc:\n",
        "        cont = True\n",
        "        ret_val.append(conc[bigram])\n",
        "      elif bigram[0] in conc:\n",
        "        ret_val.append(conc[bigram[0]])\n",
        "\n",
        "    return ret_val\n",
        "\n",
        "  def get_num_lemmas(x):\n",
        "    num_lemmas = 0.0\n",
        "\n",
        "    for token in x:\n",
        "      if token not in string.punctuation:\n",
        "        num_lemmas += 1.0\n",
        "\n",
        "    return num_lemmas\n",
        "      \n",
        "  spark.udf.register('get_d_chall', get_d_chall, FloatType())\n",
        "  spark.udf.register('get_aoa', get_aoa, ArrayType(FloatType()))\n",
        "  spark.udf.register('get_conc_rating', get_conc_rating, ArrayType(FloatType()))\n",
        "  spark.udf.register('get_num_lemmas', get_num_lemmas, FloatType())\n",
        "  spark.udf.register('get_joined_text', lambda x: ' '.join(x), StringType())\n",
        "  spark.udf.register('array_mean', lambda x: float(np.mean(x)), FloatType())\n",
        "\n",
        "  query = r\"\"\"\n",
        "  SELECT \n",
        "    original_text, \n",
        "    get_joined_text(lemma.result) as lemmatized_text, \n",
        "    get_d_chall(lemma.result) as d_chall_score,\n",
        "    get_aoa(lemma.result) as aoa,\n",
        "    get_conc_rating(lemma.result) as conc_rating,\n",
        "    get_num_lemmas(lemma.result) as num_lemmas,\n",
        "    label\n",
        "  FROM wiki;\n",
        "  \"\"\"\n",
        "  df = spark.sql(query)\n",
        "  df.createOrReplaceTempView('wiki')\n",
        "\n",
        "  spark.udf.register('array_mean', lambda x: float(np.mean(x)), FloatType())\n",
        "\n",
        "  query = r\"\"\"\n",
        "  SELECT \n",
        "  original_text, \n",
        "  lemmatized_text,\n",
        "  d_chall_score,\n",
        "  array_mean(aoa) as aoa_mean, \n",
        "  array_min(aoa) as aoa_min, \n",
        "  array_max(aoa) as aoa_max, \n",
        "  array_mean(conc_rating) as conc_rating_mean, \n",
        "  array_min(conc_rating) as conc_rating_min, \n",
        "  array_max(conc_rating) as conc_rating_max,\n",
        "  num_lemmas,\n",
        "  label\n",
        "  FROM wiki; \n",
        "  \"\"\"\n",
        "  df = spark.sql(query)\n",
        "  df.createOrReplaceTempView('wiki')\n",
        "  return df.toPandas()"
      ],
      "metadata": {
        "id": "WnpcW0-uqlvH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = get_clean_data_frame(data_path='/content/WikiLarge_Train.csv')\n",
        "df_train.shape"
      ],
      "metadata": {
        "id": "2RAXbfS7aQVP",
        "outputId": "7e43a6aa-c9e5-4dd6-9683-fe7d646d8332",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(416768, 11)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = get_clean_data_frame(data_path='/content/WikiLarge_Test.csv')\n",
        "df_test.shape"
      ],
      "metadata": {
        "id": "UDJSBvWSuWu6",
        "outputId": "8099fdb5-881b-4cb5-e271-62e49e4f15ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(119092, 11)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "clf = joblib.load('/content/drive/MyDrive/milestone-ii/Models/svc-model_final.joblib')"
      ],
      "metadata": {
        "id": "0uEoZTX1v53S"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = clf.fit(df_train, df_train.label)"
      ],
      "metadata": {
        "id": "EkCVZxECwFT5"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_hat = clf.predict(df_test)"
      ],
      "metadata": {
        "id": "bDlOv5VPwLOs"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission = pd.DataFrame(y_hat, columns=['label']).reset_index().rename({'index':'id'})"
      ],
      "metadata": {
        "id": "zjarrurE1O1p"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission.to_csv('/content/drive/MyDrive/milestone-ii/submission.csv', index=False)"
      ],
      "metadata": {
        "id": "GRnjwyq01z1N"
      },
      "execution_count": 19,
      "outputs": []
    }
  ]
}