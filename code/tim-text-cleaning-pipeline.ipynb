{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOSCT5N4tpaPDzbQGfuo0Fz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tstrebe2/predicting-text-difficulty/blob/tim-updates/code/tim-text-cleaning-pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "!{sys.executable} -m pip install pyspark==3.1.2 -q\n",
        "!{sys.executable} -m pip install spark-nlp==4.2.0 -q"
      ],
      "metadata": {
        "id": "R6Z4jjikrL1o"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StringType\n",
        "from pyspark.ml import Pipeline\n",
        "import pyspark.sql.functions as F\n",
        "from sparknlp.base import DocumentAssembler, EmbeddingsFinisher\n",
        "from sparknlp.annotator import SentenceDetector, Tokenizer, Normalizer, Lemmatizer\n",
        "import sparknlp\n",
        "from pyspark.sql.types import StringType, ArrayType, FloatType, StructType\n",
        "import numpy as np\n",
        "import string\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "sWsZSCvaR0sM"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!wget https://raw.githubusercontent.com/Tstrebe2/predicting-text-difficulty/main/assets/WikiLarge_Train.csv -q\n",
        "!wget https://raw.githubusercontent.com/mahavivo/vocabulary/master/lemmas/AntBNC_lemmas_ver_001.txt -q\n",
        "!wget https://raw.githubusercontent.com/Tstrebe2/predicting-text-difficulty/main/assets/dale_chall.txt -q\n",
        "!wget https://raw.githubusercontent.com/Tstrebe2/predicting-text-difficulty/main/assets/AoA_51715_words.csv -q\n",
        "!wget https://raw.githubusercontent.com/Tstrebe2/predicting-text-difficulty/main/assets/Concreteness_ratings_Brysbaert_et_al_BRM.txt -q\n",
        "# !wget http://nlp.stanford.edu/data/glove.6B.zip -q\n",
        "# !unzip glove*.zip"
      ],
      "metadata": {
        "id": "yY036731rSpx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec556914-0eab-42b5-fed6-f1788d8e3344"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:4.2.0\")\\\n",
        "        .getOrCreate()"
      ],
      "metadata": {
        "id": "Lu6IvgEzSATw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Readability Datasets (Provided by MADS)"
      ],
      "metadata": {
        "id": "U07c08R0bxDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_readability_datasets():\n",
        "  aoa = pd.read_csv('/content/AoA_51715_words.csv', \n",
        "                      encoding_errors='ignore', \n",
        "                      usecols=['Lemma_highest_PoS', 'AoA_Kup_lem'],\n",
        "                      ).rename({'Lemma_highest_PoS':'lemma', 'AoA_Kup_lem':'aoa'}, axis=1)\n",
        "\n",
        "  aoa = aoa.groupby('lemma').first().to_dict()['aoa']\n",
        "\n",
        "  conc = (pd.read_csv('/content/Concreteness_ratings_Brysbaert_et_al_BRM.txt', \n",
        "                    sep='\\t',\n",
        "                    usecols=['Word', 'Bigram', 'Conc.M'])\n",
        "          .rename({'Word':'word', 'Bigram':'bigram', 'Conc.M':'conc_mean'}, axis=1))\n",
        "\n",
        "  def split_word(x):\n",
        "    if x['bigram'] == 0:\n",
        "      word_or_phrase = x['word']\n",
        "    else:\n",
        "      word_or_phrase = tuple(x['word'].split(' '))\n",
        "\n",
        "    return {'word':word_or_phrase, 'conc_mean':x['conc_mean'] }\n",
        "\n",
        "  conc = conc.apply(split_word, axis=1, result_type='expand').set_index('word').to_dict()['conc_mean']\n",
        "\n",
        "  d_chall = set(pd.read_csv('/content/dale_chall.txt', names=['word'])['word'].tolist())\n",
        "  return aoa, conc, d_chall"
      ],
      "metadata": {
        "id": "5w0HXmK1bOCY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Core Datasets\n",
        "* Standars unit-conversion metrics\n",
        "* Clean punctuation\n",
        "* Lower-case\n",
        "* Lemmatize\n",
        "* Create readability feature representations using MADS provided datasets\n"
      ],
      "metadata": {
        "id": "OOEVdvltb27o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_clean_data_frame(data_path):\n",
        "  aoa, conc, d_chall = get_readability_datasets()\n",
        "\n",
        "  df = spark.read.csv(data_path, header=True)\n",
        "  df.createOrReplaceTempView('wiki')\n",
        "\n",
        "  regex1, replace1 = r\" km \", \"kilometers\"\n",
        "  regex2, replace2 = r\"[0-9]+(km) \", \"kilometers\"\n",
        "  regex3, replace3 = r\" mph \",\" miles per hour \"\n",
        "  regex4, replace4 = r\"° C \",\"degrees celsius\"\n",
        "  regex5, replace5 = r\"° F \",\"degrees farenheit\"\n",
        "  regex6, replace6 = r\"°\",\"degrees\"\n",
        "  regex7, replace7 = r\" %\",\" percent\"\n",
        "  regex8, replace8 = r\" cm\",\" centimeters\"\n",
        "  regex9, replace9 = r\" kg \",\" kilograms \"\n",
        "\n",
        "  iterable = ((regex1, replace1), (regex2, replace2), (regex3, replace3),\n",
        "              (regex4, replace4), (regex5, replace5), (regex6, replace6),\n",
        "              (regex7, replace7), (regex8, replace8), (regex9, replace9),)\n",
        "\n",
        "  for regex, replace in iterable:\n",
        "    query = f\"\"\"\n",
        "    SELECT\n",
        "      regexp_replace(original_text, '{regex}', '{replace}') as original_text, \n",
        "      label \n",
        "    FROM wiki;\"\"\"\n",
        "    df = spark.sql(query)\n",
        "    df.createOrReplaceTempView('wiki')\n",
        "\n",
        "  documentAssembler = DocumentAssembler()\\\n",
        "      .setInputCol(\"original_text\")\\\n",
        "      .setOutputCol(\"document\")\n",
        "\n",
        "  tokenizer = Tokenizer() \\\n",
        "      .setInputCols([\"document\"]) \\\n",
        "      .setOutputCol(\"token\")\n",
        "\n",
        "  normalizer = Normalizer() \\\n",
        "      .setInputCols([\"token\"]) \\\n",
        "      .setOutputCol(\"normalized\") \\\n",
        "      .setLowercase(True) \\\n",
        "      .setCleanupPatterns([\"\"\"[^\\w\\d\\s]\"\"\"]) # remove punctuations (keep alphanumeric chars)\n",
        "  # if we don't set CleanupPatterns, it will only keep alphabet letters ([^A-Za-z])\n",
        "\n",
        "  lemmatizer = Lemmatizer() \\\n",
        "      .setInputCols([\"normalized\"]) \\\n",
        "      .setOutputCol(\"lemma\") \\\n",
        "      .setDictionary(\"./AntBNC_lemmas_ver_001.txt\", value_delimiter =\"\\t\", key_delimiter = \"->\")\n",
        "\n",
        "  nlp_pipeline = Pipeline(stages=[documentAssembler,\n",
        "                                  tokenizer,\n",
        "                                  normalizer,\n",
        "                                  lemmatizer,])\n",
        "\n",
        "  nlp_pipeline = nlp_pipeline.fit(df)\n",
        "  df = nlp_pipeline.transform(df)\n",
        "  df.createOrReplaceTempView('wiki')\n",
        "\n",
        "  def get_d_chall(x):\n",
        "    easy_count = 0\n",
        "    word_count = 0\n",
        "\n",
        "    for token in x:\n",
        "      if token in string.punctuation:\n",
        "        continue\n",
        "\n",
        "      token_lower = token.lower()\n",
        "\n",
        "      if token_lower in d_chall:\n",
        "        easy_count += 1\n",
        "\n",
        "      word_count += 1\n",
        "\n",
        "    difficult_count = (word_count-easy_count)\n",
        "\n",
        "    return (0.1579 * ((difficult_count/word_count) * 100) + 0.0496 * word_count\n",
        "            if word_count else 0.0)\n",
        "\n",
        "\n",
        "  def get_aoa(x):\n",
        "    arr = [aoa[w.lower()] for w in x if w.lower() in aoa]\n",
        "    if len(arr) > 0:\n",
        "      return arr\n",
        "    else:\n",
        "      return [0.0] \n",
        "\n",
        "  def get_conc_rating(x):\n",
        "    ret_val = []\n",
        "\n",
        "    bigrams = [(f.lower(), s.lower()) for f, s in zip(x[:-1], x[1:])]\n",
        "    cont = False\n",
        "    \n",
        "    for bigram in bigrams:\n",
        "      if cont:\n",
        "        cont = False\n",
        "        continue\n",
        "\n",
        "      if bigram in conc:\n",
        "        cont = True\n",
        "        ret_val.append(conc[bigram])\n",
        "      elif bigram[0] in conc:\n",
        "        ret_val.append(conc[bigram[0]])\n",
        "\n",
        "    return ret_val\n",
        "\n",
        "  def get_num_lemmas(x):\n",
        "    num_lemmas = 0.0\n",
        "\n",
        "    for token in x:\n",
        "      if token not in string.punctuation:\n",
        "        num_lemmas += 1.0\n",
        "\n",
        "    return num_lemmas\n",
        "      \n",
        "  spark.udf.register('get_d_chall', get_d_chall, FloatType())\n",
        "  spark.udf.register('get_aoa', get_aoa, ArrayType(FloatType()))\n",
        "  spark.udf.register('get_conc_rating', get_conc_rating, ArrayType(FloatType()))\n",
        "  spark.udf.register('get_num_lemmas', get_num_lemmas, FloatType())\n",
        "  spark.udf.register('get_joined_text', lambda x: ' '.join(x), StringType())\n",
        "  spark.udf.register('array_mean', lambda x: float(np.mean(x)), FloatType())\n",
        "\n",
        "  query = r\"\"\"\n",
        "  SELECT \n",
        "    original_text, \n",
        "    get_joined_text(lemma.result) as lemmatized_text, \n",
        "    get_d_chall(lemma.result) as d_chall_score,\n",
        "    get_aoa(lemma.result) as aoa,\n",
        "    get_conc_rating(lemma.result) as conc_rating,\n",
        "    get_num_lemmas(lemma.result) as num_lemmas,\n",
        "    label\n",
        "  FROM wiki;\n",
        "  \"\"\"\n",
        "  df = spark.sql(query)\n",
        "  df.createOrReplaceTempView('wiki')\n",
        "\n",
        "  spark.udf.register('array_mean', lambda x: float(np.mean(x)), FloatType())\n",
        "\n",
        "  query = r\"\"\"\n",
        "  SELECT \n",
        "  original_text, \n",
        "  lemmatized_text,\n",
        "  d_chall_score,\n",
        "  array_mean(aoa) as aoa_mean, \n",
        "  array_min(aoa) as aoa_min, \n",
        "  array_max(aoa) as aoa_max, \n",
        "  array_mean(conc_rating) as conc_rating_mean, \n",
        "  array_min(conc_rating) as conc_rating_min, \n",
        "  array_max(conc_rating) as conc_rating_max,\n",
        "  num_lemmas,\n",
        "  label\n",
        "  FROM wiki; \n",
        "  \"\"\"\n",
        "  df = spark.sql(query)\n",
        "  df.createOrReplaceTempView('wiki')\n",
        "  return df.toPandas()"
      ],
      "metadata": {
        "id": "WnpcW0-uqlvH"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = get_clean_data_frame(data_path='/content/WikiLarge_Train.csv')\n",
        "df.shape"
      ],
      "metadata": {
        "id": "2RAXbfS7aQVP",
        "outputId": "7a4fa2c2-310a-49a4-c757-71ecf5db990d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(416768, 11)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_train_test_split(df, train_path, test_path):\n",
        "  df_test = df.groupby('label').apply(lambda grp: grp.sample(frac=.10, random_state=99)).droplevel(0, axis=0)\n",
        "  df_train = df[~df.index.isin(df_test.index)]\n",
        "\n",
        "  df_train.to_csv(train_path, sep='\\t', index=True, index_label='ix')\n",
        "  df_test.to_csv(test_path, sep='\\t', index=True, index_label='ix')\n",
        "\n",
        "save_train_test_split(df, \n",
        "                      train_path='/content/drive/MyDrive/milestone-ii/Training_set.csv',\n",
        "                      test_path='/content/drive/MyDrive/milestone-ii/Testing_set.csv')"
      ],
      "metadata": {
        "id": "fPxtmtfbYiWx"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[df.lemmatized_text.str.len() == 0]"
      ],
      "metadata": {
        "id": "RwGp-QRFhdJb",
        "outputId": "4d14c039-c04c-467b-ec2b-3a68ac42ae09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       original_text lemmatized_text  d_chall_score  aoa_mean  aoa_min  \\\n",
              "607             ! ''                            0.0       0.0      0.0   \n",
              "2118               .                            0.0       0.0      0.0   \n",
              "3821               .                            0.0       0.0      0.0   \n",
              "5949               !                            0.0       0.0      0.0   \n",
              "6570               -                            0.0       0.0      0.0   \n",
              "...              ...             ...            ...       ...      ...   \n",
              "409118             .                            0.0       0.0      0.0   \n",
              "411367             .                            0.0       0.0      0.0   \n",
              "414899             .                            0.0       0.0      0.0   \n",
              "414941             !                            0.0       0.0      0.0   \n",
              "415573             !                            0.0       0.0      0.0   \n",
              "\n",
              "        aoa_max  conc_rating_mean  conc_rating_min  conc_rating_max  \\\n",
              "607         0.0               NaN              NaN              NaN   \n",
              "2118        0.0               NaN              NaN              NaN   \n",
              "3821        0.0               NaN              NaN              NaN   \n",
              "5949        0.0               NaN              NaN              NaN   \n",
              "6570        0.0               NaN              NaN              NaN   \n",
              "...         ...               ...              ...              ...   \n",
              "409118      0.0               NaN              NaN              NaN   \n",
              "411367      0.0               NaN              NaN              NaN   \n",
              "414899      0.0               NaN              NaN              NaN   \n",
              "414941      0.0               NaN              NaN              NaN   \n",
              "415573      0.0               NaN              NaN              NaN   \n",
              "\n",
              "        num_lemmas label  \n",
              "607            0.0     1  \n",
              "2118           0.0     1  \n",
              "3821           0.0     1  \n",
              "5949           0.0     1  \n",
              "6570           0.0     1  \n",
              "...            ...   ...  \n",
              "409118         0.0     0  \n",
              "411367         0.0     0  \n",
              "414899         0.0     0  \n",
              "414941         0.0     0  \n",
              "415573         0.0     0  \n",
              "\n",
              "[293 rows x 11 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2f4f2e53-8789-4190-b45a-f2cea7e6aa01\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>original_text</th>\n",
              "      <th>lemmatized_text</th>\n",
              "      <th>d_chall_score</th>\n",
              "      <th>aoa_mean</th>\n",
              "      <th>aoa_min</th>\n",
              "      <th>aoa_max</th>\n",
              "      <th>conc_rating_mean</th>\n",
              "      <th>conc_rating_min</th>\n",
              "      <th>conc_rating_max</th>\n",
              "      <th>num_lemmas</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>607</th>\n",
              "      <td>! ''</td>\n",
              "      <td></td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2118</th>\n",
              "      <td>.</td>\n",
              "      <td></td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3821</th>\n",
              "      <td>.</td>\n",
              "      <td></td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5949</th>\n",
              "      <td>!</td>\n",
              "      <td></td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6570</th>\n",
              "      <td>-</td>\n",
              "      <td></td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>409118</th>\n",
              "      <td>.</td>\n",
              "      <td></td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>411367</th>\n",
              "      <td>.</td>\n",
              "      <td></td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>414899</th>\n",
              "      <td>.</td>\n",
              "      <td></td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>414941</th>\n",
              "      <td>!</td>\n",
              "      <td></td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>415573</th>\n",
              "      <td>!</td>\n",
              "      <td></td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>293 rows × 11 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2f4f2e53-8789-4190-b45a-f2cea7e6aa01')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2f4f2e53-8789-4190-b45a-f2cea7e6aa01 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2f4f2e53-8789-4190-b45a-f2cea7e6aa01');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Embeddings"
      ],
      "metadata": {
        "id": "Xe7Oo16cblsh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://edumunozsala.github.io/BlogEms/jupyter/nlp/classification/embeddings/python/2020/08/15/Intro_NLP_WordEmbeddings_Classification.html\n",
        "# https://radimrehurek.com/gensim_3.8.3/models/keyedvectors.html\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "root_path = '/content/'\n",
        "glove_file_name = 'glove.6B.100d.txt'\n",
        "glove_path = ''.join([root_path, glove_file_name])\n",
        "\n",
        "word2vec_output_file = ''.join([glove_file_name, '.word2vec'])\n",
        "glove2word2vec(glove_path, word2vec_output_file)\n",
        "\n",
        "model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)\n",
        "\n",
        "def get_sentence_embedding(sent):\n",
        "  sentence_embedding = np.zeros((0, 100))\n",
        "  tokens = sent.split(' ')\n",
        "  \n",
        "  for token in tokens:\n",
        "    token_lower = token.lower()\n",
        "\n",
        "    if token_lower in model:\n",
        "      word_vector = model.get_vector(token_lower)\n",
        "      sentence_embedding = np.vstack((sentence_embedding, word_vector))\n",
        "\n",
        "  if sentence_embedding.shape[0] == 0:\n",
        "    return np.zeros(100)\n",
        "  else:\n",
        "    return sentence_embedding.mean(axis=0)"
      ],
      "metadata": {
        "id": "8KLwE8Vzlzg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = np.array(df['original_text'].apply(get_sentence_embedding).tolist())\n",
        "embeddings.shape"
      ],
      "metadata": {
        "id": "dE8o2AwO5zpV",
        "outputId": "507ec776-a7bf-4d5f-e05b-1268e820a7e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(395169, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "y = df['label'].astype(np.int64)\n",
        "X_train, X_test, y_train, y_test = train_test_split(embeddings, y, test_size=.1, random_state=99)\n",
        "\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "metadata": {
        "id": "ENONlbJiswJ_",
        "outputId": "d962bd79-d261-4755-8396-971e6f937d0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((355652, 100), (39517, 100), (355652,), (39517,))"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import make_pipeline \n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "clf = make_pipeline(StandardScaler(), LogisticRegression(n_jobs=-1)).fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "U7wO3MqutVeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "y_hat = clf.predict(X_test)\n",
        "y_proba = clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "roc_auc = roc_auc_score(y_test, y_proba)\n",
        "accuracy = accuracy_score(y_test, y_hat)\n",
        "f1 = f1_score(y_test, y_hat)\n",
        "\n",
        "print('ROC AUC:', roc_auc)\n",
        "print('Accuracy:', accuracy)\n",
        "print('F1:', f1)"
      ],
      "metadata": {
        "id": "MeSOHK0Mtrui",
        "outputId": "2307a06c-3cf8-4df6-b271-8753115c1b23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC AUC: 0.6662212002745456\n",
            "Accuracy: 0.6354986461522889\n",
            "F1: 0.6687669594812123\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df[['lemmatized_text']], y, test_size=.1, random_state=99)\n",
        "\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "metadata": {
        "id": "pdTfICT3LO6X",
        "outputId": "c7c3192a-4374-427e-b6cd-15bfe931322f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((355652, 1), (39517, 1), (355652,), (39517,))"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "clf = make_pipeline(TfidfVectorizer(min_df=25, stop_words='english'), \n",
        "                    LogisticRegression(n_jobs=-1)).fit(X_train['lemmatized_text'], y_train)"
      ],
      "metadata": {
        "id": "x8gX0SbZK64W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_hat = clf.predict(X_test['lemmatized_text'])\n",
        "y_proba = clf.predict_proba(X_test['lemmatized_text'])[:, 1]\n",
        "\n",
        "roc_auc = roc_auc_score(y_test, y_proba)\n",
        "accuracy = accuracy_score(y_test, y_hat)\n",
        "f1 = f1_score(y_test, y_hat)\n",
        "\n",
        "print('ROC AUC:', roc_auc)\n",
        "print('Accuracy:', accuracy)\n",
        "print('F1:', f1)"
      ],
      "metadata": {
        "id": "_kmPOhfSLiA-",
        "outputId": "9cd07113-f6e6-42be-fe81-2b4ee937369d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC AUC: 0.730130701783159\n",
            "Accuracy: 0.6654098236202141\n",
            "F1: 0.6759631408685423\n"
          ]
        }
      ]
    }
  ]
}